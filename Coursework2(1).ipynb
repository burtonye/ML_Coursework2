{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2\n",
    "\n",
    "### Deadline: 16th December 2019, 4pm\n",
    "### Submission format\n",
    "Solve all the question directly in this notebook. Some tasks require writing and running code. Other times you are asked to write answers to questions within the 'Markdown' cells and the answers are propmted by  __Answer:__. \n",
    "\n",
    "Submit the solved Jupyter notebooks with the code and answers to KEATS. Make sure that all your code is running.You do not need to submit the data files with your notebooks. Your submission should consists of a single solved Python notebook file named  __CW2_Surname_Forname.ipynb__. \n",
    "\n",
    "Marks obtained in this coursework will be converted to __25% of your final grade__.\n",
    "\n",
    "Any questions about the coursework should be sent to maria.murgasova@kcl.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "**[Total: 30 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast cancer dataset\n",
    "\n",
    "The breast cancer dataset is available in `sklearn` and can be loaded and explored using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sklearn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bf73b20bd72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_breast_cancer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sklearn"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "print(data.keys())\n",
    "print('\\n Features: \\n', data.feature_names)\n",
    "print('\\n Labels: ', data.target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we create our usual `Features` and `Labels` that are `numpy` arrays and scale the features using `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "Features = StandardScaler().fit_transform(data.data)\n",
    "Labels = data.target\n",
    "\n",
    "print('Features dim: ', Features.shape)\n",
    "print('Labels dim: ', Labels.shape)\n",
    "print('We have {} samples and {} features.'.format(Features.shape[0],Features.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore structure of the data\n",
    "\n",
    "**Task 1.1 [3 marks]**: Use PCA to reduce the features to two dimensions and plot the reduced data highlighting the labels. To improve visualisation, make the points in the plots transparent by setting `alpha` to 0.5. Are there clear clusters in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Perform PCA\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components =2)\n",
    "principalComponents=pca.fit_transform(Features)\n",
    "principalDf = pd.DataFrame(data = principalComponents,\n",
    "                          columns = ['PC1', 'PC2'])\n",
    "#PCA_data = np.append(principalDf, Labels[:,np.newaxis],axis=1)    keeping in np.array\n",
    "PCA_data = pd.concat([principalDf,pd.DataFrame({'Labels':Labels})], axis =1)\n",
    "# Plot reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "targets = [0, 1]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = PCA_data['Labels'] == target\n",
    "    ax.scatter(PCA_data.loc[indicesToKeep, 'PC1']\n",
    "               , PCA_data.loc[indicesToKeep, 'PC2']\n",
    "               , c = color\n",
    "               , alpha = 0.5\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "\n",
    "## While we can generally see two clusters, they are not distinct and have significant overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classification\n",
    "\n",
    "**Task 1.2 [4 marks]:** Perform classification using Random Forest and calculate cross-validated accuracy. Extract and display the two most important features, including their names and importance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate a random forest classifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# Select and fit the model\n",
    "model = RandomForestRegressor(n_estimators=2)\n",
    "model.fit(Features,Labels)\n",
    "\n",
    "# Calculate CV RMSE\n",
    "scores = cross_val_score(model, Features, Labels, cv=10, scoring = 'neg_mean_squared_error')\n",
    "print('Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and print the two most important features\n",
    "\n",
    "ind = np.argpartition(model.feature_importances_,-2)[-2:]\n",
    "print(data.feature_names[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3 [6 marks]:** Visualise the results of the random forest classification. Perform following steps:\n",
    "* Perform PCA to reduce features to two dimensions\n",
    "* Calculate the 2D feature range for the reduced features\n",
    "* Predict the classification result for the 2D feature range and plot using `contourf`. *Hint: you will need to look up a method* `PCA.inverse_transform` *in sklearn help to predict the labels for the 2D feature range.*\n",
    "* Plot the reduced data with the labels highlighted on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features using PCA\n",
    "pca = PCA(n_components =2)\n",
    "principalComponents=pca.fit_transform(Features)\n",
    "principalDf = pd.DataFrame(data = principalComponents,columns = ['PC1', 'PC2'])\n",
    "np.max(principalDf['PC1'])   #-5.56 to 16.3\n",
    "np.min(principalDf['PC2'])   #-7.78 to 12.57\n",
    "\n",
    "# Generate feature space\n",
    "\n",
    "plt.scatter(principalDf.loc[:,'PC1'],principalDf.loc[:,'PC2'])\n",
    "\n",
    "# Predict and plot labels for the features space\n",
    "\n",
    "\n",
    "# Plot reduced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network classification\n",
    "**Task 1.4  [4 marks]**: Neural Network has been trained and it was found that multi-layer perceptron with two hidden layers with `3` and `2` nodes and L2 norm regularisation parameter `1` result in optimal performance. Train the network with these parameters, calculate cross-validated accuracy and visualise the decision boundary as in Task 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and train the model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1, hidden_layer_sizes=(3, 2), random_state=1)\n",
    "\n",
    "clf.fit(Features, Labels) \n",
    "\n",
    "# Calculate CV accuracy\n",
    "# Plot result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.5: [4 marks]** Visualise the coefficients of all the layers using `bar` plots. Print out number of coefficients in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "**Task 1.6 [9 marks]**: You are asked to develop a simple test for detection of breast cancer that could be used in clinical practice. The requirements are \n",
    "* There should be as few measurements as possible\n",
    "* The method for prediction of breast cancer should be as simple as possible.\n",
    "* Accuracy needs to be as high as possible.\n",
    "\n",
    "Using feature selection methods that were covered in the lectures find the smallest number of features for predictions while preserving the accuracy as much as possible. Develop a test by training a linear classifier. Display the results of the classification, including the decision boundary, to visually assess the test. Print out the names of the selected features.\n",
    "\n",
    "Decribe your new test and how you arrived at the solution. Explain why this new test is suitable. Keep your description brief.\n",
    "\n",
    "*Hint: When reducing number of features don't look for highest performance, but rather smallest number of features for which performance does not drop significantly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "**[Total: 20 marks]**\n",
    "\n",
    "### Brain MRI\n",
    "\n",
    "The code below loads two images - T1 and T2 weighted MRI. In this question you will implement 2D Gaussian Mixture Model for segmentation of this multi-channel MRI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  images \n",
    "T1 = plt.imread('T1.png')\n",
    "T2 = plt.imread('T2.png')\n",
    "\n",
    "# display images\n",
    "plt.figure(figsize = [10,4])\n",
    "plt.set_cmap('gray')\n",
    "plt.subplot(121)\n",
    "plt.imshow(T1)\n",
    "plt.title('T1', fontsize = 14)\n",
    "plt.subplot(122)\n",
    "plt.title('T2', fontsize = 14)\n",
    "plt.imshow(T2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1: [2 marks]** Calculate and plot 2D histogram of the multi-channel MRI. *Hint: matplotlib package has a suitable function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(T1.ravel(), bins=256, range=(0.0,1.0),fc='k',ec='k')\n",
    "plt.title(\"T1 Image Histogram\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(T2.ravel(), bins=256, range=(0.0,1.0),fc='k',ec='k')\n",
    "plt.title(\"T2 Image Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM segmentation\n",
    "**Task 2.2 [6 marks]:** Given the histogram above, decide how many clusters you need to segment the image. Perform GMM clustering using 2D feature space composed of these two images. Display the segmentation. Decide whether the segmentation worked by visual assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "from sklearn.mixture import GaussianMixture\n",
    "model=GaussianMixture(n_components=4)\n",
    "\n",
    "# Create feature matrix\n",
    "T1features2D = T1.reshape(-1,1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(T1features2D)\n",
    "predicted_labels=model.predict(T1features2D)\n",
    "\n",
    "# Display segmentation\n",
    "plt.imshow(predicted_labels.reshape(256,213)) #256 by 213 is T1 shape\n",
    "plt.set_cmap('plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "from sklearn.mixture import GaussianMixture\n",
    "model=GaussianMixture(n_components=4)\n",
    "\n",
    "# Create feature matrix\n",
    "T2features2D = T2.reshape(-1,1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(T2features2D)\n",
    "predicted_labels=model.predict(T2features2D)\n",
    "\n",
    "# Display segmentation\n",
    "plt.imshow(predicted_labels.reshape(256,213)) #256 by 213 is T2 shape\n",
    "plt.set_cmap('plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3: [6 marks]** Predict and display posterior probability maps for all classes. Create a plot with as many subplots as classes and display one posterior probability map in each. Display a title with the name of the tissue represented by that propability map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilistic segmentations\n",
    "proba_T1 = model.predict_proba(T1features2D)\n",
    "\n",
    "# Plot all probability maps\n",
    "for i in range(proba_T1.shape[1]):\n",
    "    # take only posteriors for class i\n",
    "    post = proba_T1[:,i]\n",
    "\n",
    "    # reshape to the 3D image\n",
    "    post2D = post.reshape(256,213)\n",
    "    \n",
    "    # display\n",
    "    plt.subplot(1,proba_T1.shape[1],i+1)\n",
    "    plt.imshow(post2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilistic segmentations\n",
    "proba_T2 = model.predict_proba(T2features2D)\n",
    "\n",
    "# Plot all probability maps\n",
    "for i in range(proba_T2.shape[1]):\n",
    "    # take only posteriors for class i\n",
    "    post = proba_T2[:,i]\n",
    "\n",
    "    # reshape to the 3D image\n",
    "    post2D = post.reshape(256,213)\n",
    "    \n",
    "    # display\n",
    "    plt.subplot(1,proba_T2.shape[1],i+1)\n",
    "    plt.imshow(post2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4 [6 marks]**: Predict likelihood function $p(y|\\phi)$ for the intensity ranges of the two images. Display the likelihood next to the histogram (in a figure with two subplots) and compare. Is that what you expected? Try to reason why the likelihood might differ from the histogram. Which classes had PDFs with behaviour that was not expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_range = np.linspace(0,np.max(T1),200)\n",
    "proba_curves = model.predict_proba(int_range.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "plt.figure(figsize = [14,10])\n",
    "# plot histogram\n",
    "plt.subplot(211)\n",
    "h2 = plt.hist(T1, bins = 100, density = True)\n",
    "plt.title('Histogram')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot posterior probabilities\n",
    "plt.subplot(212)\n",
    "plt.title('Posterior probabilities')\n",
    "plt.xlabel('intensity')\n",
    "plt.ylabel('posterior probability')\n",
    "for i in range(0,3):\n",
    "    plt.plot(int_range,proba_curves[:,i], linewidth = 3, label = 'Class {}'.format(i))\n",
    "plt.legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "**[Total: 20 marks]**\n",
    "\n",
    "## Random forest from scratch\n",
    "\n",
    "In the lectures we created our own Decision Tree and Bagging classification methods which we implemented in `DecisionTree.py` and `Bagging.py`. Random forest performs bootstrapping and aggregation just like bagging, but on top of that it also performs random selection of features at each node in the decision tree. In this question you will extend the Decision Tree and Bagging functions to create your own Random Forest.\n",
    "\n",
    "**Task 3.1 [2 marks]**  In the box below write what is the effect selecting a random subset of features at each node in terms of reducing the bias or variance of the model and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "The first thing we need to do is import our original `DecisionTree` and `Bagging` modules. We can call the functions from these imported modules using the ```DT.``` or ```BG.``` syntax. Run the code bellow to import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DecisionTree as DT\n",
    "import Bagging as BG\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing random forests behaves very similarly to that of bagged ensembles of decision trees, with the exception that when optimising for the best split of the data at each node in function `get_best_split`, only a subset of the features is considered. To do that we will write a new fuction `get_feature_subset` where the maximum proportion of features each node is controlled by the variable `max_f`. \n",
    "\n",
    "We will also need to modify several other functions to support the new functionality. The functions that don't need to be modified can be called directly from the imported modules using `DT.` and `BG.` Think very carefully which versions of the functions you need to use to avoid mistakes and loosing marks.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will test the new functions on the simulated classification dataset. We will create a two-class problem, creating a dataset with 1000 samples and 10 features, of which 3 will be informative (necessary for the classification) and none redundant. Run the code bellow to create the dataset.\n",
    "\n",
    "More info on the simulated dataset here: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets \n",
    "\n",
    "# Build a classification task using 3 informative features\n",
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_informative=3, n_classes=2)\n",
    "\n",
    "print('Number of samples: ', X.shape[0])\n",
    "print('Number of features: ', X.shape[1])\n",
    "print('Size of the label vector: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subsets of features\n",
    "\n",
    "**Task 3.2 [4 marks]:** Write a function `get_feature_subset` which, given a number of features `n_features` and the proportion of features to be sampled `max_f`, will return an array `indices`, indicating which features have been selected. This is achieved by random sampling *without replacement*. Complete the code bellow to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code for the function\n",
    "def get_feature_subset(n_features,max_f):\n",
    "    \"\"\"       \n",
    "        Returns indices of a random subset of features\n",
    "        input:\n",
    "            n_features: number of features\n",
    "            max_f: the proportion of features available for each node\n",
    "                             \n",
    "        output:\n",
    "            indices: list of selected features \n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the number of selected features (we were given a proportion of all features)\n",
    "    n_selected = None\n",
    "    # generate indices of randomly selected features (without replacement)\n",
    "    indices = None\n",
    "     \n",
    "    return indices\n",
    "\n",
    "# Test function\n",
    "ind = get_feature_subset(X.shape[1],0.3)\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update get_best_split\n",
    "\n",
    "**Task 3.3 [4 marks]**:  Now edit function ```get_best_split``` below to:\n",
    "1. Call `get_feature_subset` and have it return a random subset features with proportion `max_f`, but only if `max_f` is less than 1.0\n",
    "2. Edit the outer loop (variable `index`) such that it loops only over this subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(dataset, max_f=1.0):\n",
    "    \"\"\"\n",
    "        Search through all attributes and all possible thresholds to find the best split for the data\n",
    "        input:\n",
    "            dataset = array (n_samples,n_features+1) \n",
    "                    rows are examples \n",
    "                    last column indicates class membership\n",
    "                    remaining columns reflect features/attributes of data\n",
    "            max_f: the proportion of features available for each node\n",
    "                             \n",
    "        output:\n",
    "            dict containing: 1) 'index' : index of feature used for splittling on\n",
    "                             2)  'value': value of threshold split on\n",
    "                             3) 'branches': tuple of data arrays reflecting the optimal split into left and right branches\n",
    "                             \n",
    "    \"\"\"\n",
    "    # Extract number of features\n",
    "    n_features = dataset.shape[1]-1\n",
    "\n",
    "#### AMEND CODE HERE ###\n",
    "    # get random feature indices\n",
    "    if max_f < 1.0:\n",
    "        features = None \n",
    "    else:\n",
    "        features=np.arange(n_features)\n",
    "    print('Selected features:', features)\n",
    "        \n",
    "    # identify which labels we have\n",
    "    class_values=np.unique(dataset[:,-1])\n",
    "    \n",
    "    # initalise optimal values prior to refinment\n",
    "    best_cost=sys.float_info.max # initialise to max float\n",
    "    best_value=sys.float_info.max # initialise to max float\n",
    "    best_index=dataset.shape[1]+1 # initialise as greater than total number of features\n",
    "    best_split=tuple() # the best_split variable should contain the output of test_split that corresponds to the optimal cost\n",
    "\n",
    "    \n",
    "#### AMEND CODE HERE ###\n",
    "    # iterate over all selected features/attributes (columns of dataset)\n",
    "    for index in np.arange(n_features): \n",
    "\n",
    "        # Trialling splits defined by each row value for this attribute\n",
    "        for r_index,row in enumerate(dataset):\n",
    "            branches=DT.test_split(index, row[index], dataset)\n",
    "\n",
    "            cost=DT.split_cost(branches,class_values)\n",
    "            if cost < best_cost:\n",
    "                best_cost=cost\n",
    "                best_split=branches\n",
    "                best_index=index\n",
    "                best_value=row[index]\n",
    "                \n",
    "                \n",
    "    return {'index':best_index, 'value':best_value, 'branches':best_split}\n",
    "\n",
    "# Test function\n",
    "dataset = np.concatenate([X,y.reshape(-1,1)],axis=1)\n",
    "ans = get_best_split(dataset, 0.3)\n",
    "print('Best index: ',ans['index'])\n",
    "print('Best value: ',round(ans['value'],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update remaining functions\n",
    "\n",
    "Now we need to make sure that the argument `max_f` is passed to all functions that need it. Bellow is the updated function `run_split`. Run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split(node, max_depth, min_size, depth, max_f=1):\n",
    "     \n",
    "    \"\"\"\n",
    "        Recursively splits nodes until termination criterion is met\n",
    "        input:\n",
    "            node = dict containing: 1) 'index' : index of feature used for splittling on\n",
    "                             2)  'value': value of threshold split on\n",
    "                             3) 'branches': tuple of data arrays reflecting the optimal split into left and right branches\n",
    "            max_depth: int determining max allowable depth for the tree\n",
    "            min_size : int determining minimum number of examples allowed for any branch\n",
    "            depth: current depth of tree \n",
    "            max_f: the proportion of features available for each node            \n",
    "            \n",
    "            \n",
    "        Output:\n",
    "            node: is returned by value and returns a recursion of dicts representing the structure of the whole tree\n",
    "    \"\"\"\n",
    "    left, right = node['branches']\n",
    "    del(node['branches'])\n",
    "    # check for whether all data has been assigned to one branch; if so assign both branches the same label\n",
    "    if left.shape[0]==0 :\n",
    "        node['left'] = node['right'] = DT.to_terminal(right)       \n",
    "        return\n",
    "    if right.shape[0]==0 :\n",
    "        node['left'] = node['right'] = DT.to_terminal(left)       \n",
    "        return\n",
    "    # check for max depth; if exceeded then estimate labels for both branches\n",
    "    if max_depth != None and depth >= max_depth:\n",
    "        node['left'], node['right'] = DT.to_terminal(left), DT.to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "        # in first instance check whether the number of examples reaching the left node are less than the allowed limit\n",
    "        # if so assign as a terminal node, if not then split again\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = DT.to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_best_split(left,max_f)\n",
    "        run_split(node['left'], max_depth, min_size, depth+1,max_f)\n",
    "    \n",
    "    # process right child as for left\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = DT.to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_best_split(right,max_f)\n",
    "        run_split(node['right'], max_depth, min_size, depth+1,max_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4 [ 3 marks]**: Your task is now to edit the function `build_tree`  \n",
    "* to include an optional parameter `max_f`, which allows the user to define a maximum proportion of features to be sampled from at each node\n",
    "\n",
    "* to supply `max_f` to all functions which call `get_best_split` (including itself). \n",
    "\n",
    "Edit each line of code where this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(train, max_depth=None, min_size=1):\n",
    "    \"\"\"\n",
    "    Builds and returns final decision tree\n",
    "    \n",
    "    input:\n",
    "        train : training data array (n_samples,n_features+1)\n",
    "        max_depth: user defined max tree depth (int)\n",
    "        min_size: user defined minimum number of examples per tree tree depth (int)\n",
    "        max_f: the proportion of features available for each node  \n",
    "    \"\"\"\n",
    "    # create a root node split by calling get_best_split on the full training set\n",
    "    root = get_best_split(train) \n",
    "    # now build the tree using run_split\n",
    "    run_split(root, max_depth, min_size, 1) \n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Forest\n",
    "\n",
    "Now what remains is to create our forest of trees. \n",
    "\n",
    "Random Forests are exactly the same as bagging _EXCEPT_ that as well as creating bootstrapped samples of examples from the dataset they also randomly sample subsets of features at each node. This means we can continue to use the functions we built for our `Bagging` method but with small edits to ensure that they pass the user defined parameter `max_f` to our new `DecisionTree` functions\n",
    "\n",
    "**Task 3.5 [2 mark]** Edit function `create_bagged_ensemble` to support the new user defined argument `max_f`\n",
    "\n",
    "*Hint:* here you will need to call the build_tree function you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bagged_ensemble(data, max_depth, min_size, n_trees, random_state=42): # 1 mark\n",
    "    \n",
    "    ''' Create a bagged ensemble of decision trees\n",
    "    input:\n",
    "        data: (n_samples,n_features+1) data array\n",
    "        max_depth: max depth of trees\n",
    "        min_size: minimum number of samples allowed in tree leaf nodes\n",
    "        n_trees: total number of trees in the ensemble\n",
    "        random_state: fixes random seed\n",
    "    output:\n",
    "        bagged_ensemble: list of decision trees that make up the bagged ensemble\n",
    "    '''\n",
    "    \n",
    "    bagged_ensemble=[]\n",
    "    for i in range(n_trees):\n",
    "        print('building tree', i)\n",
    "        sample = BG.bootstrap_sample(data, random_state)\n",
    "        tree = None\n",
    "        bagged_ensemble.append(tree)\n",
    "    \n",
    "    return bagged_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "The code bellow creates a training and test dataset. Run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "dataset_train=np.concatenate((X_train,np.expand_dims(y_train,axis=1)),axis=1)\n",
    "dataset_test=np.concatenate((X_test,np.expand_dims(y_test,axis=1)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate the performance of our new Random Forest and compare it to the performance of a single decision tree. To calculate accuracy, we will predict the labels of the test set using the unchanged funcions `predict` that are already implemented in `Bagging` and `DecisionTree` modules.\n",
    "\n",
    "**Task 3.6 [ 2 marks]**  Build a single decision tree using your newly implemented function and set the parameters as `max_depth=3`, `min_size=1`, `max_f=1.0`. Report the accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the tree\n",
    "tree = None\n",
    "\n",
    "# Predict on test set\n",
    "predictions = None\n",
    "\n",
    "# Calculate and print accuracy\n",
    "print('Accuracy:', round(DT.score(y_test,predictions),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.7 [ 3 marks]**  Build and train your newly implemeted Random Forest with parameters `max_depth=3`, `min_size=1`, `max_f=0.3`, n_trees=10. Report the accuracy on the test set. \n",
    "\n",
    "Note: This might take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the forest\n",
    "forest = None\n",
    "\n",
    "# Predict on test set\n",
    "rf_predictions = None\n",
    "\n",
    "# Calculate and print accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "**[Total: 20 marks]**\n",
    "\n",
    "## Detecting cancer from histopatological images\n",
    "In this question we will apply feature extractors to classify histopatological images for presence of cancer. More details about the PatchCamelyon dataset can be found here https://github.com/basveeling/pcam.\n",
    "<img src=\"pcam.jpg\" style=\"max-width:100%; width: 100%; max-width: none\">\n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "Run the code bellow to load the dataset from the file `histological_data.npz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from .npz file\n",
    "data = np.load('histological_data.npz')\n",
    "\n",
    "# Train images and labels\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "# Test images and labels\n",
    "X_test  = data['X_test']\n",
    "y_test  = data['y_test']\n",
    "\n",
    "# Print shapes here\n",
    "print('Training data - images:', X_train.shape)\n",
    "print('Training data - labels:',y_train.shape)\n",
    "print('Test data - images:',X_test.shape)\n",
    "print('Test data - labels:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot a few example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_images = [4, 5, 6, 7]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in np.arange(0, 4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(X_train[id_images[i], :, :], cmap='gray')\n",
    "    plt.title('label: ' + str(y_train[id_images[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "In the lectures we have seen a number of feature extractors that are available at `skimage`, including `daisy`. Bellow is an example of feature extraction using daisy. Use the code bellow to experiment with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "\n",
    "# example image\n",
    "img = X_train[id_images[i], :, :]\n",
    "\n",
    "# example feature extraction using daisy\n",
    "features_daisy, visualisation_daisy = daisy(img, step=32, radius=8, rings=2, histograms=8, orientations=8, visualize=True)\n",
    "plt.imshow(visualisation_daisy)\n",
    "plt.title('Daisy')\n",
    "# Extracted features\n",
    "print('Feature vector shape daisy: ', features_daisy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1 [4 marks]:** In the function bellow implement a feature extractor using `daisy`. The function should accepts a 2D image and return a vector of features. Don't forget to make sure that the feature vector is flattened to 1 dimension. You can start with the parameter setting given above, but you will change the parameters later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_feature_extractor(image):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code bellow to perform feature extraction for the whole dataset. It will take a while for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature extraction on the train/test set\n",
    "\n",
    "# Create empty arrays\n",
    "X_train_features = []\n",
    "X_test_features  = []\n",
    "\n",
    "# Go through all the images, perform feature extraction and then append them to the list\n",
    "for img in X_train:\n",
    "    X_train_features.append(my_feature_extractor(img))\n",
    "for img in X_test:\n",
    "    X_test_features.append(my_feature_extractor(img))\n",
    "    \n",
    "# Make the lists back into numpy arrays\n",
    "X_train_features = np.asarray(X_train_features)\n",
    "X_test_features  = np.asarray(X_test_features)\n",
    "\n",
    "# Print dimensions\n",
    "print('Feature matrix train: ', X_train_features.shape)\n",
    "print('Feature matrix test: ', X_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 4.2 [4 marks]:__ In the box bellow explain the meaning of the dimensions of the two matrices, printed by the code in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 4.3 [8 marks]:__ Bellow you are given a function `PerformanceMeasures` to calculate the performance of a classifier. Furthermore, the features of the training set have been scaled using the `StandardScaler`. You are also given a `RandomForestClassifier` model with parameter `min_samples_leaf = 50` to prevent overfitting due to a large number of extracted features. Complete the code bellow to\n",
    "* fit the `RandomForestClassifier` model to the training data\n",
    "* calculate the performance measures on the training set to spot overfitting\n",
    "* calculate the performance measures on the test set to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def PerformanceMeasures(model,Features,Labels):\n",
    "    Labels_predicted = model.predict(Features)\n",
    "    tn, fp, fn, tp = confusion_matrix(Labels, Labels_predicted).ravel()\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    print('Accuracy: ', round(accuracy,2))\n",
    "    print('Sensitivity: ', round(sensitivity,2))\n",
    "    print('Specificity: ', round(specificity,2))\n",
    "    print('Precision: ', round(precision,2))\n",
    "\n",
    "\n",
    "Features = StandardScaler().fit_transform(X_train_features)\n",
    "Labels = y_train\n",
    "\n",
    "# Fit and evaluate a random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=50, min_samples_leaf = 50) \n",
    "\n",
    "# fit the model\n",
    "\n",
    "# Calculate performance measures on training set\n",
    "print('Training set:')\n",
    "\n",
    "# Calculate performance measures on test set\n",
    "print('Test set:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 4.4 [4 marks]:__ Try to improve the performance of your classifier by changing the parameters of the feature extractor in the function `my_feature_extractor`. Once you have decided on your favourite parameters, comment on the performance of the classifier in the box bellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
